{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0cb420b25547fda99a857a852fc4ef6c9d051c43278915c32b64b662fab4bab10",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "cb420b25547fda99a857a852fc4ef6c9d051c43278915c32b64b662fab4bab10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json(\"./data/train_lem.json\")\n",
    "df_val = pd.read_json(\"./data/val_lem.json\")\n",
    "df_test = pd.read_json(\"./data/test_lem.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1)\n",
    "df_val = df_val.sample(frac=1)\n",
    "df_test = df_test.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/useless_words.txt\")\n",
    "useless_words = list(file.read().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000, stop_words=useless_words, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tfidf.fit_transform(df_train[\"text\"]).toarray()\n",
    "y_train = df_train[\"subreddit_id\"]\n",
    "\n",
    "x_val = tfidf.transform(df_val[\"text\"]).toarray()\n",
    "y_val = df_val[\"subreddit_id\"]\n",
    "\n",
    "x_test = tfidf.transform(df_test[\"text\"]).toarray()\n",
    "y_test = df_test[\"subreddit_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tfidf.get_feature_names()[idx] for idx, value in enumerate(x_test[3]) if value > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_dim=5000, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Dense(units=102, activation='softmax')\n",
    "]) \n",
    "\n",
    "ann_model.compile(optimizer='adam',\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "1913/1913 [==============================] - 36s 18ms/step - loss: 2.1574 - accuracy: 0.5922 - val_loss: 0.5061 - val_accuracy: 0.8754\n",
      "Epoch 2/3\n",
      "1913/1913 [==============================] - 27s 14ms/step - loss: 0.3070 - accuracy: 0.9238 - val_loss: 0.4994 - val_accuracy: 0.8763\n",
      "Epoch 3/3\n",
      "1913/1913 [==============================] - 25s 13ms/step - loss: 0.1418 - accuracy: 0.9657 - val_loss: 0.5417 - val_accuracy: 0.8727\n"
     ]
    }
   ],
   "source": [
    "h = ann_model.fit(x_train, \n",
    "              y_train,\n",
    "              epochs=3,\n",
    "              batch_size=32,\n",
    "              validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "638/638 [==============================] - 4s 6ms/step - loss: 0.7191 - accuracy: 0.8444\n",
      "test loss, test acc: [0.7191479802131653, 0.844362735748291]\n"
     ]
    }
   ],
   "source": [
    "results = ann_model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = np.round(ann_model.predict(x_test[:100]))\n",
    "for i in range(len(predictions)):\n",
    "    print(df_test[\"text\"].values[i][:100],\"...\")\n",
    "    print(\"Pred: \", predictions[i], \"Real: \", y_test[i])"
   ]
  },
  {
   "source": [
    "-------------------------------------------------------------------\n",
    "RNN\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wget\n",
    "#url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "#path=\"./data/\"\n",
    "#filename = wget.download(url, out=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#with zipfile.ZipFile(filename, \"r\") as fzip:\n",
    "#    fzip.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(\"./data/glove.6B.50d.txt\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        word, vector = line.split(maxsplit=1)\n",
    "        vector = np.fromstring(vector, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Converted 4903 words (97 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(tfidf.vocabulary_) + 2\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tfidf.vocabulary_.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_indices(x_old):\n",
    "    x_new = []\n",
    "    for row in x_old:\n",
    "        e = enumerate(row)\n",
    "        for i in e:\n",
    "            result = []\n",
    "            for idx, value in e:\n",
    "                if value > 0.1:\n",
    "                    result.append(idx)\n",
    "            x_new.append(result)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#x_train_rnn = []\n",
    "#for row in x_train:\n",
    "#    e = enumerate(row)\n",
    "#    for i in e:\n",
    "#        result = []\n",
    "#        for idx, value in e:\n",
    "#            if value > 0.1:\n",
    "#                result.append(idx)\n",
    "#        x_train_rnn.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rnn = get_word_indices(x_train)\n",
    "x_val_rnn = get_word_indices(x_val)\n",
    "x_test_rnn = get_word_indices(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 64\n",
    "x_train_rnn = tf.keras.preprocessing.sequence.pad_sequences(x_train_rnn, maxlen=maxlen)\n",
    "x_val_rnn = tf.keras.preprocessing.sequence.pad_sequences(x_val_rnn, maxlen=maxlen)\n",
    "x_test_rnn = tf.keras.preprocessing.sequence.pad_sequences(x_test_rnn, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_37\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_34 (Embedding)     (None, 64, 50)            250100    \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 64)                29440     \n_________________________________________________________________\ndense_53 (Dense)             (None, 102)               6630      \n=================================================================\nTotal params: 286,170\nTrainable params: 286,170\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(num_tokens, \n",
    "                              embedding_dim,                               \n",
    "                              embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                              trainable=True,\n",
    "                              input_length=maxlen),\n",
    "    tf.keras.layers.LSTM(units=64, activation='tanh'),\n",
    "    tf.keras.layers.Dense(102, activation='softmax')\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer='adam',\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1913/1913 [==============================] - 112s 57ms/step - loss: 3.2445 - accuracy: 0.2513 - val_loss: 1.2704 - val_accuracy: 0.6977\n",
      "Epoch 2/5\n",
      "1913/1913 [==============================] - 118s 61ms/step - loss: 1.0009 - accuracy: 0.7613 - val_loss: 0.7950 - val_accuracy: 0.8065\n",
      "Epoch 3/5\n",
      "1913/1913 [==============================] - 131s 68ms/step - loss: 0.6277 - accuracy: 0.8451 - val_loss: 0.6942 - val_accuracy: 0.8278\n",
      "Epoch 4/5\n",
      "1913/1913 [==============================] - 117s 61ms/step - loss: 0.4709 - accuracy: 0.8802 - val_loss: 0.6709 - val_accuracy: 0.8335\n",
      "Epoch 5/5\n",
      "1913/1913 [==============================] - 114s 59ms/step - loss: 0.3772 - accuracy: 0.9045 - val_loss: 0.6616 - val_accuracy: 0.8374\n"
     ]
    }
   ],
   "source": [
    "h1 = rnn_model.fit(x_train_rnn, \n",
    "              y_train,\n",
    "              epochs=5,\n",
    "              batch_size=32,\n",
    "              validation_data=(x_val_rnn, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "638/638 [==============================] - 11s 17ms/step - loss: 0.7849 - accuracy: 0.8103\n",
      "test loss, test acc: [0.7849007248878479, 0.8102940917015076]\n"
     ]
    }
   ],
   "source": [
    "results = cnn_model.evaluate(x_test_rnn, y_test, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "predictions = np.round(rnn_model.predict(x_test_rnn[:100]))\n",
    "for i in range(len(predictions)):\n",
    "    print(df_test[\"text\"].values[i][:100],\"...\")\n",
    "    print(\"Pred: \", predictions[i], \"Real: \", y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}