{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0cb420b25547fda99a857a852fc4ef6c9d051c43278915c32b64b662fab4bab10",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "cb420b25547fda99a857a852fc4ef6c9d051c43278915c32b64b662fab4bab10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html as ihtml\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/rspct_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"selftext\"]\n",
    "del df[\"title\"]\n",
    "del df[\"selftext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = len(df)\n",
    "classes = len(df[\"subreddit\"].unique())\n",
    "\n",
    "len_train = int(round(len_df * 0.6))\n",
    "len_val_test = (len_df - len_train) // (2 * classes)\n",
    "len_train = len_train // classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(columns=df.columns)\n",
    "for i in range(0, len(df[\"subreddit_id\"].unique())+1):\n",
    "    df_train = df_train.append(df.loc[df['subreddit_id'] == i][:len_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.DataFrame(columns=df.columns)\n",
    "for i in range(0, classes+1):\n",
    "    df_val = df_val.append(df.loc[df['subreddit_id'] == i][len_train:len_train+len_val_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(columns=df.columns)\n",
    "for i in range(0, classes+1):\n",
    "    df_test = df_test.append(df.loc[df['subreddit_id'] == i][len_train+len_val_test:], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    '''Html tagek és linkek eltávolítása'''\n",
    "    text = str(text)\n",
    "    text = BeautifulSoup(ihtml.unescape(text)).text\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_accented_chars(text):\n",
    "    '''Az 'é' betűs szavak átalakítása'''\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation(text):\n",
    "    '''Írásjelek eltávolítása'''\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Stopword-ök eltávolítása'''\n",
    "    tokenized_text = text.split(' ')\n",
    "    return ' '.join([w for w in tokenized_text if not w in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''Lemmatizáció'''\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([w.lemma_ for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train dataset tisztítása'''\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(clean_html)\n",
    "df_train['text'] = df_train['text'].apply(clean_accented_chars)\n",
    "df_train['text'] = df_train['text'].apply(clean_punctuation)\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lemmatization)\n",
    "df_train[\"text\"] = df_train[\"text\"].str.lower()\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Val dataset megtisztítása'''\n",
    "\n",
    "df_val['text'] = df_val['text'].apply(clean_html)\n",
    "df_val['text'] = df_val['text'].apply(clean_accented_chars)\n",
    "df_val['text'] = df_val['text'].apply(clean_punctuation)\n",
    "df_val[\"text\"] = df_val[\"text\"].apply(lemmatization)\n",
    "df_val[\"text\"] = df_val[\"text\"].str.lower()\n",
    "df_val[\"text\"] = df_val[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json(\"./data/train_lem.json\", orient=\"records\")\n",
    "df_val.to_json(\"./data/val_lem.json\", orient=\"records\")\n",
    "df_test.to_json(\"./data/test_lem.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}