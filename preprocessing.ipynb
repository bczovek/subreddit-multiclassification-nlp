{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd096eb50e1d44aed467dc8f759cb08c32fbfa9babcf79c554e2d0e5feb04653a10",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cb420b25547fda99a857a852fc4ef6c9d051c43278915c32b64b662fab4bab10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html as ihtml\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/rspct_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['title'] + ' ' + df['selftext']\n",
    "del df['title']\n",
    "del df['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 102000 entries, 0 to 101999\nData columns (total 4 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   id            102000 non-null  object\n 1   subreddit     102000 non-null  object\n 2   subreddit_id  102000 non-null  int64 \n 3   text          102000 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 3.1+ MB\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id     subreddit  subreddit_id  \\\n",
       "0     6cxb31  13ReasonsWhy             0   \n",
       "1     8mr3cw  13ReasonsWhy             0   \n",
       "2     8lyac3  13ReasonsWhy             0   \n",
       "3     8kfqoc  13ReasonsWhy             0   \n",
       "4     64koz8  13ReasonsWhy             0   \n",
       "...      ...           ...           ...   \n",
       "1195  7vvbq1          ADHD             1   \n",
       "1196  4qtyse          ADHD             1   \n",
       "1197  5mnzux          ADHD             1   \n",
       "1198  8jy3y8          ADHD             1   \n",
       "1199  79k0qu          ADHD             1   \n",
       "\n",
       "                                                   text  \n",
       "0     A problem I had with Beyond the Reasons [Spoil...  \n",
       "1     Has anyone noticed these similarities? (Discus...  \n",
       "2     Bryce Walker vs Brock Turner Anybody else noti...  \n",
       "3     Mr. Porter respect (only through second episod...  \n",
       "4     Past v Present Anyone else notice that when we...  \n",
       "...                                                 ...  \n",
       "1195  Need help, more information I can’t seem to fi...  \n",
       "1196  Forgetting where you put stuff? I left my keys...  \n",
       "1197  Forced Amphetamine Withdrawal: Zero Hour Im do...  \n",
       "1198  Adderall brighter light, hard to focus I am cu...  \n",
       "1199  My long term boyfriend was recently diagnosed ...  \n",
       "\n",
       "[1200 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>subreddit</th>\n      <th>subreddit_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6cxb31</td>\n      <td>13ReasonsWhy</td>\n      <td>0</td>\n      <td>A problem I had with Beyond the Reasons [Spoil...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8mr3cw</td>\n      <td>13ReasonsWhy</td>\n      <td>0</td>\n      <td>Has anyone noticed these similarities? (Discus...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8lyac3</td>\n      <td>13ReasonsWhy</td>\n      <td>0</td>\n      <td>Bryce Walker vs Brock Turner Anybody else noti...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8kfqoc</td>\n      <td>13ReasonsWhy</td>\n      <td>0</td>\n      <td>Mr. Porter respect (only through second episod...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>64koz8</td>\n      <td>13ReasonsWhy</td>\n      <td>0</td>\n      <td>Past v Present Anyone else notice that when we...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>7vvbq1</td>\n      <td>ADHD</td>\n      <td>1</td>\n      <td>Need help, more information I can’t seem to fi...</td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>4qtyse</td>\n      <td>ADHD</td>\n      <td>1</td>\n      <td>Forgetting where you put stuff? I left my keys...</td>\n    </tr>\n    <tr>\n      <th>1197</th>\n      <td>5mnzux</td>\n      <td>ADHD</td>\n      <td>1</td>\n      <td>Forced Amphetamine Withdrawal: Zero Hour Im do...</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>8jy3y8</td>\n      <td>ADHD</td>\n      <td>1</td>\n      <td>Adderall brighter light, hard to focus I am cu...</td>\n    </tr>\n    <tr>\n      <th>1199</th>\n      <td>79k0qu</td>\n      <td>ADHD</td>\n      <td>1</td>\n      <td>My long term boyfriend was recently diagnosed ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1200 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.info()\n",
    "df.head(1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dataset felosztása'''\n",
    "\n",
    "len_df = len(df)\n",
    "classes = len(df['subreddit'].unique())\n",
    "\n",
    "len_train = int(round(len_df * 0.6))\n",
    "len_val_test = (len_df - len_train) // (2 * classes)\n",
    "len_train = len_train // classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(columns=df.columns)\n",
    "for i in range(0, len(df['subreddit_id'].unique())+1):\n",
    "    df_train = df_train.append(df.loc[df['subreddit_id'] == i][:len_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.DataFrame(columns=df.columns)\n",
    "for i in range(0, classes+1):\n",
    "    df_val = df_val.append(df.loc[df['subreddit_id'] == i][len_train:len_train+len_val_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(columns=df.columns)\n",
    "for i in range(0, classes+1):\n",
    "    df_test = df_test.append(df.loc[df['subreddit_id'] == i][len_train+len_val_test:], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    '''Html tagek és linkek eltávolítása'''\n",
    "    text = str(text)\n",
    "    text = BeautifulSoup(ihtml.unescape(text)).text\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_accented_chars(text):\n",
    "    '''Az 'é' betűs szavak átalakítása'''\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation(text):\n",
    "    '''Írásjelek eltávolítása'''\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''Stopword-ök eltávolítása'''\n",
    "    tokenized_text = text.split(' ')\n",
    "    return ' '.join([w for w in tokenized_text if not w in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''Lemmatizáció'''\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([w.lemma_ for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train dataset tisztítása'''\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(clean_html)\n",
    "df_train['text'] = df_train['text'].apply(clean_accented_chars)\n",
    "df_train['text'] = df_train['text'].apply(clean_punctuation)\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lemmatization)\n",
    "df_train[\"text\"] = df_train[\"text\"].str.lower()\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Val dataset megtisztítása'''\n",
    "\n",
    "df_val['text'] = df_val['text'].apply(clean_html)\n",
    "df_val['text'] = df_val['text'].apply(clean_accented_chars)\n",
    "df_val['text'] = df_val['text'].apply(clean_punctuation)\n",
    "df_val[\"text\"] = df_val[\"text\"].apply(lemmatization)\n",
    "df_val[\"text\"] = df_val[\"text\"].str.lower()\n",
    "df_val[\"text\"] = df_val[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Datasetek kiírása JSON-be'''\n",
    "\n",
    "df_train.to_json(\"./data/train_lem.json\", orient=\"records\")\n",
    "df_val.to_json(\"./data/val_lem.json\", orient=\"records\")\n",
    "df_test.to_json(\"./data/test_lem.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}